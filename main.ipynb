{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part I: kNN\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train kNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy over  100 iterations: 0.7960810810810811\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "data_df = pd.read_csv(\"NHANES_data_train.csv\")\n",
    "\n",
    "# Split the data into rows with MIs and those without\n",
    "MIs = data_df[data_df[\"MI\"] == 1]\n",
    "noMIs = data_df[data_df[\"MI\"] == 2]\n",
    "\n",
    "iterations = 100\n",
    "total = 0\n",
    "\n",
    "# Change random state each iteration keep track of the one that gives the highest\n",
    "for i in range(0, iterations):\n",
    "\n",
    "    # Data with no MIs is a larger set than that with MIs, so okay to drop NaNs\n",
    "    noMIs = noMIs.dropna()\n",
    "\n",
    "    # Fill missing data in MIs using the imputer method considering 5 neighbors\n",
    "    imputer = KNNImputer(n_neighbors=5)\n",
    "    imputer.fit(MIs)\n",
    "    imputed_data = imputer.transform(MIs)\n",
    "    imputed_df = pd.DataFrame(imputed_data, columns=MIs.columns)\n",
    "\n",
    "    undersample_noMIs = noMIs.sample(frac=0.11)\n",
    "\n",
    "    # combine datasets\n",
    "    # Ignore index to concatenate to the appropriate axis\n",
    "    data = pd.concat([imputed_df, undersample_noMIs], ignore_index=True)\n",
    "\n",
    "    # What variables make sense to include?\n",
    "    X = data[['Sex', 'Age', 'Race', 'Diastolic',\n",
    "        'Systolic', 'Pulse', 'BMI', 'HDL', 'Trig', 'LDL', 'TCHOL',\n",
    "        'kidneys_eGFR', 'Diabetes', 'CurrentSmoker', 'isActive']]\n",
    "    y = data['MI']\n",
    "\n",
    "    # Split the dataset into random training and testing sets\n",
    "    # To find random_state and n_neighbors, I ran two nested for loops which calculated accuracies for different combinations and found the highest\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=43)\n",
    "\n",
    "    # Scale the data, and choose how many neighbors to consider\n",
    "    clf2 = Pipeline(\n",
    "        steps=[(\"scaler\", StandardScaler()), (\"knn\", KNeighborsClassifier(n_neighbors=58))]\n",
    "    )\n",
    "\n",
    "    # set model parameters from training set\n",
    "    clf2.fit(X_train, y_train)\n",
    "\n",
    "    # add the accuracy to total.. used to calculate the average accuracy \n",
    "    accuracy = clf2.score(X_test, y_test)\n",
    "    total += accuracy\n",
    "\n",
    "print(\"Average accuracy over \", iterations, \"iterations:\", total / iterations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run \"NHANES_test_data_4_students.csv\" through the trained kNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the dataset into a dataframe\n",
    "data = pd.read_csv(\"NHANES_test_data_4_students.csv\")\n",
    "\n",
    "# Save participant IDS in a list for later\n",
    "ParticipantID = data['ParticipantID'].tolist()\n",
    "\n",
    "# choose the features we want to use from the dataset\n",
    "data = data[['Sex', 'Age', 'Race', 'Diastolic',\n",
    "        'Systolic', 'Pulse', 'BMI', 'HDL', 'Trig', 'LDL', 'TCHOL',\n",
    "        'kidneys_eGFR', 'Diabetes', 'CurrentSmoker', 'isActive']]\n",
    "\n",
    "# Fill missing data in using the imputer method considering 5 neighbors\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "imputer.fit(data)\n",
    "imputed_data = imputer.transform(data)\n",
    "imputed_df = pd.DataFrame(imputed_data, columns=data.columns)\n",
    "\n",
    "# run the model with this data\n",
    "new_pred = clf2.predict_proba(imputed_df)\n",
    "\n",
    "# predict_proba gives us positive and negative probabilities\n",
    "# take the probability of the POSITIVE predictions (probability of a participant suffering an MI in the near future) and place in a list\n",
    "Pred_Probability = []\n",
    "for i in new_pred:\n",
    "    Pred_Probability.append(i[1])\n",
    "\n",
    "results = pd.DataFrame({'ParticipantID': ParticipantID, 'Pred_Probability': Pred_Probability})\n",
    "results.to_csv(\"kNN_pred.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part II: Logistic Regression\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy over  100 iterations: 0.7571621621621623\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "data_df = pd.read_csv(\"NHANES_data_train.csv\")\n",
    "total = 0\n",
    "interations = 100\n",
    "\n",
    "# Split the data into rows with MIs and those without\n",
    "for i in range(0, iterations):\n",
    "    MIs = data_df[data_df[\"MI\"] == 1]\n",
    "    noMIs = data_df[data_df[\"MI\"] == 2]\n",
    "\n",
    "    # Data with no MIs is a larger set than that with MIs, so okay to drop NaNs\n",
    "    noMIs = noMIs.dropna()\n",
    "\n",
    "    # Fill missing data in MIs using the imputer method considering 5 neighbors\n",
    "    imputer = KNNImputer(n_neighbors=5)\n",
    "    imputer.fit(MIs)\n",
    "    imputed_data = imputer.transform(MIs)\n",
    "    imputed_df = pd.DataFrame(imputed_data, columns=MIs.columns)\n",
    "\n",
    "    undersample_noMIs = noMIs.sample(frac=0.11)\n",
    "\n",
    "    # combine datasets\n",
    "    # Ignore index to concatenate to the appropriate axis\n",
    "    data = pd.concat([imputed_df, undersample_noMIs], ignore_index=True)\n",
    "\n",
    "    # What variables make sense to include?\n",
    "    X = data[['Sex', 'Age', 'Race', 'Diastolic',\n",
    "        'Systolic', 'Pulse', 'BMI', 'HDL', 'Trig', 'LDL', 'TCHOL',\n",
    "        'kidneys_eGFR', 'Diabetes', 'CurrentSmoker', 'isActive']]\n",
    "    y = data['MI']\n",
    "\n",
    "    # Split the dataset into random training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=5)\n",
    "\n",
    "    logreg = LogisticRegression(C=1000, max_iter=100000).fit(X_train, y_train)\n",
    "\n",
    "    total += logreg.score(X_test, y_test)\n",
    "\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# y_predicted = logreg.predict(X_test)\n",
    "# print(\"confusion matrix:\\n\", confusion_matrix(y_test, y_predicted))\n",
    "\n",
    "# print(\"Training set score: {:.3f}\".format(logreg.score(X_train, y_train))) \n",
    "# print(\"Test set score: {:.3f}\".format(logreg.score(X_test, y_test)))\n",
    "\n",
    "print(\"Average accuracy over \", iterations, \"iterations:\", total / iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run \"NHANES_test_data_4_students.csv\" through the trained logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the dataset into a dataframe\n",
    "data = pd.read_csv(\"NHANES_test_data_4_students.csv\")\n",
    "\n",
    "# Save participant IDS in a list for later\n",
    "ParticipantID = data['ParticipantID'].tolist()\n",
    "\n",
    "# choose the features we want to use from the dataset\n",
    "data = data[['Sex', 'Age', 'Race', 'Diastolic',\n",
    "        'Systolic', 'Pulse', 'BMI', 'HDL', 'Trig', 'LDL', 'TCHOL',\n",
    "        'kidneys_eGFR', 'Diabetes', 'CurrentSmoker', 'isActive']]\n",
    "\n",
    "# Fill missing data in using the imputer method considering 5 neighbors\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "imputer.fit(data)\n",
    "imputed_data = imputer.transform(data)\n",
    "imputed_df = pd.DataFrame(imputed_data, columns=data.columns)\n",
    "\n",
    "# run the model with this data\n",
    "new_pred = logreg.predict_proba(imputed_df)\n",
    "\n",
    "# predict_proba gives us positive and negative probabilities\n",
    "# take the probability of the POSITIVE predictions (probability of a participant suffering an MI in the near future) and place in a list\n",
    "Pred_Probability = []\n",
    "for i in new_pred:\n",
    "    Pred_Probability.append(i[1])\n",
    "\n",
    "results = pd.DataFrame({'ParticipantID': ParticipantID, 'Pred_Probability': Pred_Probability})\n",
    "results.to_csv(\"regression_pred.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
